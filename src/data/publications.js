export const publications = [
  {
    title: "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
    authors: "Haoru Xue*, Tairan He*, Zi Wang*, Qingwei Ben, Wenli Xiao, Zhengyi Luo, Xingye Da, Fernando Castañeda, Guanya Shi, Shankar Sastry, Linxi \"Jim\" Fan†, Yuke Zhu†",
    year: "2025",
    link: "https://doorman-humanoid.github.io/",
    webpage: "https://doorman-humanoid.github.io/",
    pdf: "https://arxiv.org/abs/2512.01061",
    arxiv: "https://arxiv.org/abs/2512.01061",
    abstract: "Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.",
    bibtex: "@article{xue2025openingsimtorealdoorhumanoid,\n    title={Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer},\n    author={Haoru Xue and Tairan He and Zi Wang and Qingwei Ben and Wenli Xiao and Zhengyi Luo and Xingye Da and Fernando Castañeda and Guanya Shi and Shankar Sastry and Linxi \"Jim\" Fan and Yuke Zhu},\n    year={2025},\n    eprint={2512.01061},\n    archivePrefix={arXiv},\n    primaryClass={cs.RO},\n    url={https://arxiv.org/abs/2512.01061}\n  }",
    image: "/index_files/tairan&amp;h1_crop.JPG",
    video: "https://tairanhe.com/images/doorman/doorman-teaser-576P.mp4",
  },
  {
    title: "VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation",
    authors: "Tairan He*, Zi Wang*, Haoru Xue*, Qingwei Ben*, Zhengyi Luo, Wenli Xiao, Ye Yuan, Xingye Da, Fernando Castañeda, Shankar Sastry, Changliu Liu, Guanya Shi, Linxi \"Jim\" Fan†, Yuke Zhu†",
    year: "2025",
    link: "https://viral-humanoid.github.io/",
    webpage: "https://viral-humanoid.github.io/",
    pdf: "https://arxiv.org/abs/2511.15200",
    arxiv: "https://arxiv.org/abs/2511.15200",
    abstract: "A key barrier to the real-world deployment of humanoid robots is the lack of autonomous loco-manipulation skills. We introduce VIRAL, a visual sim-to-real framework that learns humanoid loco-manipulation entirely in simulation and deploys it zero-shot to real hardware. VIRAL follows a teacher-student design: a privileged RL teacher, operating on full state, learns long-horizon loco-manipulation using a delta action space and reference state initialization. A vision-based student policy is then distilled from the teacher via large-scale simulation with tiled rendering, trained with a mixture of online DAgger and behavior cloning. We find that compute scale is critical: scaling simulation to tens of GPUs (up to 64) makes both teacher and student training reliable, while low-compute regimes often fail. To bridge the sim-to-real gap, VIRAL combines large-scale visual domain randomization over lighting, materials, camera parameters, image quality, and sensor delays—with real-to-sim alignment of the dexterous hands and cameras. Deployed on a Unitree G1 humanoid, the resulting RGB-based policy performs continuous loco-manipulation for up to 54 cycles, generalizing to diverse spatial and appearance variations without any real-world fine-tuning, and approaching expert-level teleoperation performance. Extensive ablations dissect the key design choices required to make RGB-based humanoid loco-manipulation work in practice.",
    bibtex: "@article{he2025viral,\n    title={VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation},\n    author={He, Tairan and Wang, Zi and Xue, Haoru and Ben, Qingwei and Luo, Zhengyi and Xiao, Wenli and Yuan, Ye and Da, Xingye and Castañeda, Fernando and Sastry, Shankar and Liu, Changliu and Shi, Guanya and Fan, Linxi and Zhu, Yuke},\n    journal={arXiv preprint arXiv:2511.15200},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/viral/viral-for-preview-v2-576P.mp4",
  },
  {
    title: "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
    authors: "Zhengyi Luo*, Ye Yuan*, Tingwu Wang*, Chenran Li*, Sirui Chen, Fernando Castañeda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi ``Jim\" Fan†, Yuke Zhu†",
    year: "2025",
    link: "https://nvlabs.github.io/SONIC/",
    webpage: "https://nvlabs.github.io/SONIC/",
    pdf: "https://arxiv.org/abs/2511.07820",
    arxiv: "https://arxiv.org/abs/2511.07820",
    abstract: "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
    bibtex: "@article{luo2025sonic,\n    title={SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control},\n    author={Luo, Zhengyi and Yuan, Ye and Wang, Tingwu and Li, Chenran and Chen, Sirui and Casta\\~neda, Fernando and Cao, Zi-Ang and Li, Jiefeng and Minor, David and Ben, Qingwei and Da, Xingye and Ding, Runyu and Hogg, Cyrus and Song, Lina and Lim, Edy and Jeong, Eugene and He, Tairan and Xue, Haoru and Xiao, Wenli and Wang, Zi and Yuen, Simon and Kautz, Jan and Chang, Yan and Iqbal, Umar and Fan, Linxi and Zhu, Yuke},\n    journal={arXiv preprint arXiv:2511.07820},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/sonic/sonic-preview_576P.mp4",
  },
  {
    title: "HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos",
    authors: "Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, Guanya Shi",
    year: "2025",
    link: "https://hdmi-humanoid.github.io/#/",
    webpage: "https://hdmi-humanoid.github.io/#/",
    pdf: "https://arxiv.org/abs/2509.16757",
    arxiv: "https://arxiv.org/abs/2509.16757",
    code: "https://github.com/LeCAR-Lab/HDMI",
    abstract: "Enabling robust whole-body humanoid-object interaction (HOI) remains challenging due to motion data scarcity and the contact-rich nature. We present HDMI (HumanoiD iMitation for Interaction), a simple and general framework that learns whole-body humanoid-object interaction skills directly from monocular RGB videos. Our pipeline (i) extracts and retargets human and object trajectories from unconstrained videos to build structured motion datasets, (ii) trains a reinforcement learning (RL) policy to co-track robot and object states with three key designs: a unified object representation, a residual action space, and a general interaction reward, and (iii) zero-shot deploys the RL policies on real humanoid robots. Extensive sim-to-real experiments on a Unitree G1 humanoid demonstrate the robustness and generality of our approach: HDMI achieves 67 consecutive door traversals and successfully performs 6 distinct loco-manipulation tasks in the real world and 14 tasks in simulation. Our results establish HDMI as a simple and general framework for acquiring interactive humanoid skills from human videos.",
    bibtex: "@article{weng2025hdmi,\n    title={HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos},\n    author={Weng, Haoyang and Li, Yitang and Sobanbabu, Nikhil and Wang, Zihan and Luo, Zhengyi and He, Tairan and Ramanan, Deva and Shi, Guanya},\n    journal={arXiv preprint arXiv:2509.16757},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/hdmi/suitcase-D9lPef67.mp4",
  },
  {
    title: "FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation",
    authors: "Yuanhang Zhang, Yifu Yuan, Prajwal Gurunath, Tairan He, Shayegan Omidshafiei, Ali-akbar Agha-mohammadi, Marcell Vazquez-Chanlatte, Liam Pedersen, Guanya Shi",
    year: "2025",
    link: "https://lecar-lab.github.io/falcon-humanoid/",
    webpage: "https://lecar-lab.github.io/falcon-humanoid/",
    pdf: "https://arxiv.org/abs/2505.06776",
    arxiv: "https://arxiv.org/abs/2505.06776",
    code: "https://github.com/LeCAR-Lab/FALCON",
    abstract: "Humanoid loco-manipulation holds transformative potential for daily service and industrial tasks, yet achieving precise, robust whole-body control with 3D end-effector force interaction remains a major challenge. Prior approaches are often limited to lightweight tasks or quadrupedal/wheeled platforms. To overcome these limitations, we propose FALCON, a dual-agent reinforcement-learning-based framework for robust force-adaptive humanoid loco-manipulation. FALCON decomposes whole-body control into two specialized agents: (1) a lower-body agent ensuring stable locomotion under external force disturbances, and (2) an upper-body agent precisely tracking end-effector positions with implicit adaptive force compensation. These two agents are jointly trained in simulation with a force curriculum that progressively escalates the magnitude of external force exerted on the end effector while respecting torque limits. Experiments demonstrate that, compared to the baselines, FALCON achieves 2x more accurate upper-body joint tracking, while maintaining robust locomotion under force disturbances and achieving faster training convergence. Moreover, FALCON enables policy training without embodiment-specific reward or curriculum tuning. Using the same training setup, we obtain policies that are deployed across multiple humanoids, enabling forceful loco-manipulation tasks such as transporting payloads (0-20N force), cart-pulling (0-100N), and door-opening (0-40N) in the real world.",
    bibtex: "@article{zhang2025falcon,\n    title={FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation},\n    author={Zhang, Yuanhang and Yuan, Yifu and Gurunath, Prajwal and He, Tairan and Omidshafiei, Shayegan and Agha-mohammadi, Ali-akbar and Vazquez-Chanlatte, Marcell and Pedersen, Liam and Shi, Guanya},\n    journal={arXiv preprint arXiv:2505.06776},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/falcon/heavy_lifting_falcon_mask_h264.mp4",
  },
  {
    title: "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning",
    authors: "Zhengyi Luo, Chen Tessler, Toru Lin, Ye Yuan, Tairan He, Wenli Xiao, Yunrong Guo, Gal Chechik, Kris Kitani, Linxi Fan, Yuke Zhu",
    year: "2025",
    link: "https://www.zhengyiluo.com/PDC-Site/",
    webpage: "https://www.zhengyiluo.com/PDC-Site/",
    pdf: "https://arxiv.org/abs/2505.12278",
    arxiv: "https://arxiv.org/abs/2505.12278",
    abstract: "Human behavior is fundamentally shaped by visual perception -- our ability to interact with the world depends on actively gathering relevant information and adapting our movements accordingly. Behaviors like searching for objects, reaching, and hand-eye coordination naturally emerge from the structure of our sensory system. Inspired by these principles, we introduce Perceptive Dexterous Control (PDC), a framework for vision-driven dexterous whole-body control with simulated humanoids. PDC operates solely on egocentric vision for task specification, enabling object search, target placement, and skill selection through visual cues, without relying on privileged state information (e.g., 3D object positions and geometries). This perception-as-interface paradigm enables learning a single policy to perform multiple household tasks, including reaching, grasping, placing, and articulated object manipulation. We also show that training from scratch with reinforcement learning can produce emergent behaviors such as active search. These results demonstrate how vision-driven control and complex tasks induce human-like behaviors and can serve as the key ingredients in closing the perception-action loop for animation, robotics, and embodied AI.",
    bibtex: "@article{luo2025emergent,\n    title={Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning},\n    author={Luo, Zhengyi and Tessler, Chen and Lin, Toru and Yuan, Ye and He, Tairan and Xiao, Wenli and Guo, Yunrong and Chechik, Gal and Kitani, Kris and Fan, Linxi and others},\n    journal={arXiv preprint arXiv:2505.12278},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/pdc/kitchen.mp4",
  },
  {
    title: "Humanoid Policy ~ Human Policy",
    authors: "Ri-Zhao Qiu*, Shiqi Yang*, Xuxin Cheng*, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, David J. Yoon, Ryan Hoque, Lars Paulsen, Ge Yang, Jian Zhang, Sha Yi, Guanya Shi, Xiaolong Wang",
    year: "2025",
    link: "https://human-as-robot.github.io/",
    webpage: "https://human-as-robot.github.io/",
    pdf: "https://arxiv.org/pdf/2503.13441",
    arxiv: "https://arxiv.org/abs/2503.13441",
    code: "https://github.com/RogerQi/human-policy",
    abstract: "Training manipulation policies for humanoid robots with diverse data enhance their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset (PH2D) that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both generalization and robustness of HAT with significantly better data collection efficiency.",
    bibtex: "@article{qiu2025-humanpolicy,\n    title={Humanoid Policy \\~{} Human Policy},\n    author={Ri-Zhao Qiu and Shiqi Yang and Xuxin Cheng and Chaitanya Chawla and Jialong Li and Tairan He and Ge Yan and David J. Yoon and Ryan Hoque and Lars Paulsen and Ge Yang and Jian Zhang and Sha Yi and Guanya Shi and Xiaolong Wang},\n    journal={arXiv preprint arXiv:2503.13441},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/hat/hat_demo.mp4",
  },
  {
    title: "Sampling-Based System Identification with Active Exploration for Legged Robot Sim2Real Learning",
    authors: "Nikhil Sobanbabu, Guanqi He, Tairan He, Yuxiang Yang, Guanya Shi",
    year: "2025",
    link: "https://lecar-lab.github.io/spi-active_/",
    webpage: "https://lecar-lab.github.io/spi-active_/",
    pdf: "https://arxiv.org/abs/2505.14266",
    arxiv: "https://arxiv.org/abs/2505.14266",
    code: "https://github.com/LeCAR-Lab/SPI-Active",
    abstract: "Sim-to-real discrepancies hinder learning-based policies from achieving high-precision tasks in the real world. While Domain Randomization (DR) is commonly used to bridge this gap, it often relies on heuristics and can lead to overly conservative policies with degrading performance when not properly tuned. System Identification (Sys-ID) offers a targeted approach, but standard techniques rely on differentiable dynamics and/or direct torque measurement, assumptions that rarely hold for contact-rich legged systems. To this end, we present SPI-Active (Sampling-based Parameter Identification with Active Exploration), a two-stage framework that estimates physical parameters of legged robots to minimize the sim-to-real gap. SPI-Active robustly identifies key physical parameters through massive parallel sampling, minimizing state prediction errors between simulated and real-world trajectories. To further improve the informativeness of collected data, we introduce an active exploration strategy that maximizes the Fisher Information of the collected real-world trajectories via optimizing the input commands of an exploration policy. This targeted exploration leads to accurate identification and better generalization across diverse tasks. Experiments demonstrate that SPI-Active enables precise sim-to-real transfer of learned policies to the real world, outperforming baselines by 42-63% in various locomotion tasks.",
    bibtex: "@article{sobanbabu2025sampling,\n    title={Sampling-based system identification with active exploration for legged robot sim2real learning},\n    author={Sobanbabu, Nikhil and He, Guanqi and He, Tairan and Yang, Yuxiang and Shi, Guanya},\n    journal={arXiv preprint arXiv:2505.14266},\n    year={2025}\n  }",
    video: "https://tairanhe.com/images/spi-active/fj_spi-active.mp4",
  },
  {
    title: "Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control",
    authors: "Yitang Li, Yuanhang Zhang, Wenli Xiao, Chaoyi Pan, Haoyang Weng, Guanqi He, Tairan He, Guanya Shi",
    year: "2025",
    link: "https://lecar-lab.github.io/SoFTA/",
    webpage: "https://lecar-lab.github.io/SoFTA/",
    pdf: "https://arxiv.org/abs/2505.24198",
    arxiv: "https://arxiv.org/abs/2505.24198",
    code: "https://github.com/LeCAR-Lab/SoFTA",
    abstract: "Can your humanoid walk up and hand you a full cup of beer, without spilling a drop? While humanoids are increasingly featured in flashy demos like dancing, delivering packages, traversing rough terrain, fine-grained control during locomotion remains a significant challenge. In particular, stabilizing a filled end-effector (EE) while walking is far from solved, due to a fundamental mismatch in task dynamics: locomotion demands slow-timescale, robust control, whereas EE stabilization requires rapid, high-precision corrections. To address this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples upper-body and lower-body control into separate agents operating at different frequencies and with distinct rewards. This temporal and objective separation mitigates policy interference and enables coordinated whole-body behavior. SoFTA executes upper-body actions at 100 Hz for precise EE control and lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x relative to baselines and performs much closer to human-level stability, enabling delicate tasks such as carrying nearly full cups, capturing steady video during locomotion, and disturbance rejection with EE stability.",
    bibtex: "@inproceedings{li2025hold,\n    title={Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control},\n    author={Li, Yitang and Zhang, Yuanhang and Xiao, Wenli and Pan, Chaoyi and Weng, Haoyang and He, Guanqi and He, Tairan and Shi, Guanya},\n    booktitle={RSS 2025 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond}\n  }",
    video: "https://tairanhe.com/images/softa/hold-my-beer-push-1.mp4",
  },
  {
    title: "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills",
    authors: "Tairan He*, Jiawei Gao*, Wenli Xiao*, Yuanhang Zhang*, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, Zeji Yi, Guannan Qu, Kris Kitani, Jessica Hodgins, Linxi \"Jim\" Fan, Yuke Zhu, Changliu Liu, Guanya Shi",
    year: "2025",
    link: "https://agile.human2humanoid.com/",
    webpage: "https://agile.human2humanoid.com/",
    pdf: "https://agile.human2humanoid.com/static/asap.pdf",
    arxiv: "https://arxiv.org/abs/2502.01143",
    code: "https://github.com/LeCAR-Lab/ASAP",
    abstract: "Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.",
    bibtex: "@article{he2024asap,\n    author = {He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi \"Jim\" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},\n    title     = {ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},\n    booktitle = {arXiv preprint},\n    year      = {2025},\n}",
    video: "https://tairanhe.com/images/asap/asap-preview-gif-720P.mp4",
  },
  {
    title: "HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots",
    authors: "Tairan He*, Wenli Xiao*, Toru Lin, Zhengyi Luo, Zhengjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi \"Jim\" Fan†, Yuke Zhu†",
    year: "2025",
    link: "https://hover-versatile-humanoid.github.io/",
    webpage: "https://hover-versatile-humanoid.github.io/",
    pdf: "https://hover-versatile-humanoid.github.io/resources/HOVER_paper.pdf",
    arxiv: "https://arxiv.org/abs/2410.21229",
    code: "https://github.com/NVlabs/HOVER/",
    abstract: "Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.",
    bibtex: "@article{he2024hover,\n  title={HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots},\n  author={He, Tairan and Xiao, Wenli and Lin, Toru and Luo, Zhengyi and Xu, Zhenjia and Jiang, Zhenyu and Kautz, Jan and Liu, Changliu and Shi, Guanya and Wang, Xiaolong and Fan, Linxi and Zhu, Yuke},\n  journal={arXiv preprint arXiv:2410.21229},\n  year={2024}\n}",
    video: "https://tairanhe.com/images/hover/HOVER-Teaser-preview-720.mp4",
  },
  {
    title: "Bridging Adaptivity and Safety: Learning Agile Collision-Free Locomotion Across Varied Physics",
    authors: "Yichao Zhong, Chong Zhang, Tairan He, Guanya Shi",
    year: "2025",
    link: "https://adaptive-safe-locomotion.github.io/",
    webpage: "https://adaptive-safe-locomotion.github.io/",
    pdf: "https://arxiv.org/pdf/2501.04276",
    arxiv: "https://arxiv.org/abs/2501.04276",
    code: "https://github.com/z-taylcr7/BAS_official",
    abstract: "Real-world legged locomotion systems often need to reconcile agility and safety for different scenarios. Moreover, the underlying dynamics are often unknown and time-variant (e.g., payload, friction). In this paper, we introduce BAS (Bridging Adaptivity and Safety), which builds upon the pipeline of prior work Agile But Safe (ABS)(He et al.) and is designed to provide adaptive safety even in dynamic environments with uncertainties. BAS involves an agile policy to avoid obstacles rapidly and a recovery policy to prevent collisions, a physical parameter estimator that is concurrently trained with agile policy, and a learned control-theoretic RA (reach-avoid) value network that governs the policy switch. Also, the agile policy and RA network are both conditioned on physical parameters to make them adaptive. To mitigate the distribution shift issue, we further introduce an on-policy fine-tuning phase for the estimator to enhance its robustness and accuracy. The simulation results show that BAS achieves 50% better safety than baselines in dynamic environments while maintaining a higher speed on average. In real-world experiments, BAS shows its capability in complex environments with unknown physics (e.g., slippery floors with unknown frictions, unknown payloads up to 8kg), while baselines lack adaptivity, leading to collisions or. degraded agility. As a result, BAS achieves a 19.8% increase in speed and gets a 2.36 times lower collision rate than ABS in the real world.",
    bibtex: "@article{zhong2025bridging,\n  title={Bridging Adaptivity and Safety: Learning Agile Collision-Free Locomotion Across Varied Physics},\n  author={Zhong, Yichao and Zhang, Chong and He, Tairan and Shi, Guanya},\n  journal={arXiv preprint arXiv:2501.04276},\n  year={2025}\n}",
    video: "https://tairanhe.com/images/bas/bas_slope_safe_2.mp4",
  },
  {
    title: "OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning",
    authors: "Tairan He*, Zhengyi Luo*, Xialin He*, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, Guanya Shi",
    year: "2024",
    link: "https://omni.human2humanoid.com/",
    webpage: "https://omni.human2humanoid.com/",
    pdf: "https://omni.human2humanoid.com/resources/OmniH2O_paper.pdf",
    arxiv: "https://arxiv.org/abs/2406.08858",
    code: "https://github.com/LeCAR-Lab/human2humanoid",
    abstract: "We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.",
    bibtex: "@article{he2024omnih2o,\n  title={OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning},\n  author={He, Tairan and Luo, Zhengyi and He, Xialin and Xiao, Wenli and Zhang, Chong and Zhang, Weinan and Kitani, Kris and Liu, Changliu and Shi, Guanya},\n  journal={arXiv preprint arXiv:2406.08858},\n  year={2024}\n}",
    video: "https://tairanhe.com/images/omnih2o/Preview-OmniH2O.mp4",
  },
  {
    title: "WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts",
    authors: "Chong Zhang*, Wenli Xiao*, Tairan He, Guanya Shi",
    year: "",
    link: "https://lecar-lab.github.io/wococo/",
    webpage: "https://lecar-lab.github.io/wococo/",
    pdf: "https://arxiv.org/pdf/2406.06005",
    arxiv: "https://arxiv.org/abs/2406.06005",
    code: "https://github.com/LeCAR-Lab/wococo",
    abstract: "Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models. Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences. In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages. Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task. We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing. We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks.",
    bibtex: "@article{zhang2024wococo,\n    title={WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts},\n    author={Zhang, Chong and Xiao, Wenli and He, Tairan and Shi, Guanya},\n    journal={arXiv e-prints},\n    pages={arXiv--2406},\n    year={2024}\n  }",
    video: "https://tairanhe.com/images/wococo/wococo-preview.mp4",
  },
  {
    title: "Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation",
    authors: "Tairan He*, Zhengyi Luo*, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi",
    year: "",
    link: "https://human2humanoid.com/",
    webpage: "https://human2humanoid.com/",
    pdf: "https://human2humanoid.com/resources/H2O_paper.pdf",
    arxiv: "https://arxiv.org/abs/2403.04436",
    code: "https://github.com/LeCAR-Lab/human2humanoid",
    abstract: "We present <span style=\"color: Red;\">H</span>uman <span style=\"color: Red;\">to</span> Human<span style=\"color: Red;\">o</span>id (<strong>H2O</strong>), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable ''sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.",
    bibtex: "@article{he2024learning,\n  title={Learning human-to-humanoid real-time whole-body teleoperation},\n  author={He, Tairan and Luo, Zhengyi and Xiao, Wenli and Zhang, Chong and Kitani, Kris and Liu, Changliu and Shi, Guanya},\n  journal={arXiv preprint arXiv:2403.04436},\n  year={2024}\n}",
    video: "https://tairanhe.com/images/h2o/h2o-preview.mp4",
  },
  {
    title: "Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion",
    authors: "Tairan He*, Chong Zhang*, Wenli Xiao, Guanqi He, Changliu Liu, Guanya Shi",
    year: "",
    link: "https://agile-but-safe.github.io/",
    webpage: "https://agile-but-safe.github.io/",
    pdf: "https://arxiv.org/pdf/2401.17583.pdf",
    arxiv: "https://arxiv.org/abs/2401.17583",
    code: "https://github.com/LeCAR-Lab/ABS",
    abstract: "Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles.",
    bibtex: "@article{he2024agile,\n    title={Agile but safe: Learning collision-free high-speed legged locomotion},\n    author={He, Tairan and Zhang, Chong and Xiao, Wenli and He, Guanqi and Liu, Changliu and Shi, Guanya},\n    journal={arXiv preprint arXiv:2401.17583},\n    year={2024}\n  }",
    video: "https://tairanhe.com/images/agile-but-safe/abs-gif-preview-long.mp4",
  },
  {
    title: "Safe Deep Policy Adaption",
    authors: "Wenli Xiao*, Tairan He*, John Dolan, Guanya Shi",
    year: "2024",
    link: "https://sites.google.com/view/safe-deep-policy-adaptation",
    webpage: "https://sites.google.com/view/safe-deep-policy-adaptation",
    pdf: "https://arxiv.org/pdf/2310.08602.pdf",
    arxiv: "https://arxiv.org/abs/2310.08602",
    code: "https://github.com/LeCAR-Lab/SafeDPA",
    abstract: "A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors and extra perturbations. Comprehensive experiments on (1) classic control problems (Inverted Pendulum), (2) simulation benchmarks (Safety Gym), and (3) a real-world agile robotics platform (RC Car) demonstrate great superiority of SafeDPA in both safety and task performance, over state-of-the-art baselines. Particularly, SafeDPA demonstrates notable generalizability, achieving a 300% increase in safety rate compared to the baselines, under unseen disturbances in real-world experiments.",
    bibtex: "@article{xiao2023safe,\n    title={Safe Deep Policy Adaptation},\n    author={Xiao, Wenli and He, Tairan and Dolan, John and Shi, Guanya},\n    journal={arXiv preprint arXiv:2310.08602},\n    year={2023}\n  }",
    video: "https://tairanhe.com/images/safedpa/SafeDPA-showoff.mp4",
  },
  {
    title: "Progressive Adaptive Chance-Constrained Safeguards for Reinforcement Learning",
    authors: "Zhaorun Chen, Binhao Chen, Tairan He, Liang Gong, Chengliang Liu",
    year: "",
    link: "https://arxiv.org/abs/2310.03379",
    webpage: "https://manipulation-locomotion.github.io",
    pdf: "https://arxiv.org/pdf/2310.03379.pdf",
    arxiv: "https://arxiv.org/abs/2310.03379",
    abstract: "An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective and requires immense engineering to support coordination between the arm and legs, error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible where there is evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups.",
    bibtex: "@article{chen2023progressive,\n    title={Progressive Adaptive Chance-Constrained Safeguards for Reinforcement Learning},\n    author={Chen, Zhaorun and Chen, Binhao and He, Tairan and Gong, Liang and Liu, Chengliang},\n    journal={arXiv preprint arXiv:2310.03379},\n    year={2023}\n  }",
    video: "https://tairanhe.com/images/acs/ACS-Video.mp4",
  },
  {
    title: "State-wise Safe Reinforcement Learning: A Survey",
    authors: "Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, Changliu Liu",
    year: "2023",
    link: "https://arxiv.org/abs/2302.03122",
    pdf: "https://arxiv.org/pdf/2302.03122.pdf",
    arxiv: "https://arxiv.org/abs/2302.03122",
    abstract: "Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potential future directions.",
    bibtex: "@inproceedings{ijcai2023p763,\n    title     = {State-wise Safe Reinforcement Learning: A Survey},\n    author    = {Zhao, Weiye and He, Tairan and Chen, Rui and Wei, Tianhao and Liu, Changliu},\n    booktitle = {Proceedings of the Thirty-Second International Joint Conference on\n                 Artificial Intelligence, {IJCAI-23}},\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    editor    = {Edith Elkind},\n    pages     = {6814--6822},\n    year      = {2023},\n    month     = {8},\n    note      = {Survey Track},\n    doi       = {10.24963/ijcai.2023/763},\n    url       = {https://doi.org/10.24963/ijcai.2023/763},\n  }",
    image: "/index_files/saferl_survey.png",
  },
  {
    title: "Visual Imitation Learning with Patch Rewards",
    authors: "Minghuan Liu, Tairan He, Weinan Zhang, Shuicheng Yan, Zhongwen Xu\n      ICLR 2023",
    year: "2023",
    link: "https://sites.google.com/view/patchail/",
    webpage: "https://sites.google.com/view/patchail/",
    pdf: "https://arxiv.org/pdf/2302.00965.pdf",
    arxiv: "https://arxiv.org/abs/2302.00965",
    code: "https://github.com/sail-sg/PatchAIL",
    abstract: "Visual imitation learning enables reinforcement learning agents to learn to be- have from expert visual demonstrations such as videos or image sequences, with- out explicit, well-defined rewards. Previous research either adopted supervised learning techniques or induce simple and coarse scalar rewards from pixels, ne- glecting the dense information contained in the image demonstrations. In this work, we propose to measure the expertise of various local regions of image sam- ples, or called patches, and recover multi-dimensional patch rewards accordingly. Patch reward is a more precise rewarding characterization that serves as a fine- grained expertise measurement and visual explainability tool. Specifically, we present Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards. The patch-based knowledge is also used to regularize the aggregated reward and stabilize the training. We evaluate our method on DeepMind Control Suite and Atari tasks. The experiment results have demonstrated that PatchAIL outperforms baseline methods and pro- vides valuable interpretations for visual demonstrations.",
    bibtex: "@article{liu2023visual,\n    title={Visual imitation learning with patch rewards},\n    author={Liu, Minghuan and He, Tairan and Zhang, Weinan and Yan, Shuicheng and Xu, Zhongwen},\n    journal={arXiv preprint arXiv:2302.00965},\n    year={2023}\n  }",
    video: "https://tairanhe.com/images/patchail/PatchAIL-Allplay-3.mp4",
  },
  {
    title: "Safety Index Synthesis via Sum-of-Squares Programming",
    authors: "Weiye Zhao*, Tairan He*, Tianhao Wei, Simin Liu, Changliu Liu",
    year: "2023",
    link: "https://arxiv.org/abs/2302.03122",
    pdf: "https://arxiv.org/pdf/2209.09134.pdf",
    arxiv: "https://arxiv.org/abs/2209.09134",
    abstract: "Control systems often need to satisfy strict safety requirements. Safety index provides a handy way to evaluate the safety level of the system and derive the resulting safe control policies. However, designing safety index functions under control limits is difficult and requires a great amount of expert knowledge. This paper proposes a framework for synthesizing the safety index for general control systems using sum-of-squares programming. Our approach is to show that ensuring the non-emptiness of safe control on the safe set boundary is equivalent to a local manifold positiveness problem. We then prove that this problem is equivalent to sum-of-squares programming via the Positivstellensatz of algebraic geometry. We validate the proposed method on robot arms with different degrees of freedom and ground vehicles. The results show that the synthesized safety index guarantees safety and our method is effective even in high-dimensional robot systems.",
    bibtex: "@inproceedings{zhao2023safety,\n    title={Safety index synthesis via sum-of-squares programming},\n    author={Zhao, Weiye and He, Tairan and Wei, Tianhao and Liu, Simin and Liu, Changliu},\n    booktitle={2023 American Control Conference (ACC)},\n    pages={732--737},\n    year={2023},\n    organization={IEEE}\n  }",
    image: "/index_files/sisos.png",
  },
  {
    title: "Probabilistic Safeguard for Reinforcement Learning Using Safety Index Guided Gaussian Process Models",
    authors: "Weiye Zhao*, Tairan He*, Changliu Liu",
    year: "2023",
    link: "https://arxiv.org/abs/2302.03122",
    pdf: "https://arxiv.org/pdf/2210.01041.pdf",
    arxiv: "https://arxiv.org/abs/2210.01041",
    abstract: "Safety is one of the biggest concerns to applying reinforcement learning (RL) to the physical world. In its core part, it is challenging to ensure RL agents persistently satisfy a hard state constraint without white-box or black-box dynamics models. This paper presents an integrated model learning and safe control framework to safeguard any agent, where its dynamics are learned as Gaussian processes. The proposed theory provides (i) a novel method to construct an offline dataset for model learning that best achieves safety requirements; (ii) a parameterization rule for safety index to ensure the existence of safe control; (iii) a safety guarantee in terms of probabilistic forward invariance when the model is learned using the aforementioned dataset. Simulation results show that our framework guarantees almost zero safety violation on various continuous control tasks.",
    bibtex: "@inproceedings{zhao2023probabilistic,\n    title={Probabilistic safeguard for reinforcement learning using safety index guided gaussian process models},\n    author={Zhao, Weiye and He, Tairan and Liu, Changliu},\n    booktitle={Learning for Dynamics and Control Conference},\n    pages={783--796},\n    year={2023},\n    organization={PMLR}\n  }",
    image: "/index_files/uaissa.png",
  },
  {
    title: "AutoCost: Evolving Intrinsic Cost for Zero-violation Reinforcement Learning",
    authors: "Tairan He, Weiye Zhao, Changliu Liu",
    year: "2023",
    link: "https://arxiv.org/abs/2302.03122",
    pdf: "https://arxiv.org/pdf/2301.10339.pdf",
    arxiv: "https://arxiv.org/abs/2301.10339",
    abstract: "Safety is a critical hurdle that limits the application of deep reinforcement learning (RL) to real-world control tasks. To this end, constrained reinforcement learning leverages cost functions to improve safety in constrained Markov decision processes. However, such constrained RL methods fail to achieve zero violation even when the cost limit is zero. This paper analyzes the reason for such failure, which suggests that a proper cost function plays an important role in constrained RL. Inspired by the analysis, we propose AutoCost, a simple yet effective framework that automatically searches for cost functions that help constrained RL to achieve zero-violation performance. We validate the proposed method and the searched cost function on the safe RL benchmark Safety Gym. We compare the performance of augmented agents that use our cost function to provide additive intrinsic costs with baseline agents that use the same policy learners but with only extrinsic costs. Results show that the converged policies with intrinsic costs in all environments achieve zero constraint violation and comparable performance with baselines.",
    bibtex: "@article{he2023autocost,\n    title={Autocost: Evolving intrinsic cost for zero-violation reinforcement learning},\n    author={He, Tairan and Zhao, Weiye and Liu, Changliu},\n    journal={arXiv preprint arXiv:2301.10339},\n    year={2023}\n  }",
    image: "/index_files/autocost.png",
  },
  {
    title: "Reinforcement Learning with Automated Auxiliary Loss Search",
    authors: "Tairan He, Yuge Zhang, Kan Ren, Minghuan Liu, Che Wang, Weinan Zhang, Yuqing Yang, Dongsheng Li",
    year: "2022",
    link: "https://seqml.github.io/a2ls/",
    webpage: "https://seqml.github.io/a2ls/",
    pdf: "https://arxiv.org/pdf/2210.06041.pdf",
    arxiv: "https://arxiv.org/abs/2210.06041",
    code: "https://github.com/microsoft/autorl-research/tree/main/a2ls",
    abstract: "A good state representation is crucial to solving complicated reinforcement learning (RL) challenges. Many recent works focus on designing auxiliary losses for learning informative representations. Unfortunately, these handcrafted objectives rely heavily on expert knowledge and may be sub-optimal. In this paper, we propose a principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing auxiliary loss functions for RL. Specifically, based on the collected trajectory data, we define a general auxiliary loss space of size 7.5×1020 and explore the space with an efficient evolutionary search strategy. Empirical results show that the discovered auxiliary loss (namely, A2-winner) significantly improves the performance on both high-dimensional (image) and low-dimensional (vector) unseen tasks with much higher efficiency, showing promising generalization ability to different settings and even different benchmark domains. We conduct a statistical analysis to reveal the relations between patterns of auxiliary losses and RL performance.",
    bibtex: "@inproceedings{zhao2021model,\n    title={Model-free safe control for zero-violation reinforcement learning},\n    author={Zhao, Weiye and He, Tairan and Liu, Changliu},\n    booktitle={5th Annual Conference on Robot Learning},\n    year={2021}\n  }",
    image: "/index_files/a2ls.png",
  },
  {
    title: "Model-free Safe Control for Zero-Violation Reinforcement Learning",
    authors: "Weiye Zhao, Tairan He, Changliu Liu",
    year: "2021",
    link: "https://proceedings.mlr.press/v164/zhao22a.html",
    pdf: "https://proceedings.mlr.press/v164/zhao22a/zhao22a.pdf",
    code: "https://github.com/TairanHe/ISSA",
    abstract: "While deep reinforcement learning (DRL) has impressive performance in a variety of continuous control tasks, one critical hurdle that limits the application of DRL to physical world is the lack of safety guarantees. It is challenging for DRL agents to persistently satisfy a hard state constraint (known as the safety specification) during training. On the other hand, safe control methods with safety guarantees have been extensively studied. However, to synthesize safe control, these methods require explicit analytical models of the dynamic system; but these models are usually not available in DRL. This paper presents a model-free safe control strategy to synthesize safeguards for DRL agents, which will ensure zero safety violation during training. In particular, we present an implicit safe set algorithm, which synthesizes the safety index (also called the barrier certificate) and the subsequent safe control law only by querying a black-box dynamic function (e.g., a digital twin simulator). The theoretical results indicate the implicit safe set algorithm guarantees forward invariance and finite-time convergence to the safe set. We validate the proposed method on the state-of-the-art safety benchmark Safety Gym. Results show that the proposed method achieves zero safety violation and gains 95 cumulative reward compared to state-of-the-art safe DRL methods. Moreover, it can easily scale to high-dimensional systems.",
    bibtex: "@inproceedings{zhao2021model,\n    title={Model-free safe control for zero-violation reinforcement learning},\n    author={Zhao, Weiye and He, Tairan and Liu, Changliu},\n    booktitle={5th Annual Conference on Robot Learning},\n    year={2021}\n  }",
    video: "https://tairanhe.com/images/issa/Comparison.mp4",
  },
  {
    title: "Energy-Based Imitation Learning",
    authors: "Minghuan Liu, Tairan He, Minkai Xu, Weinan Zhang",
    year: "",
    link: "https://arxiv.org/abs/2302.03122",
    pdf: "https://arxiv.org/pdf/2004.09395.pdf",
    arxiv: "https://arxiv.org/abs/2004.09395",
    code: "https://github.com/apexrl/EBIL-torch",
    abstract: "A good state representation is crucial to solving complicated reinforcement learning (RL) challenges. Many recent works focus on designing auxiliary losses for learning informative representations. Unfortunately, these handcrafted objectives rely heavily on expert knowledge and may be sub-optimal. In this paper, we propose a principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing auxiliary loss functions for RL. Specifically, based on the collected trajectory data, we define a general auxiliary loss space of size 7.5×1020 and explore the space with an efficient evolutionary search strategy. Empirical results show that the discovered auxiliary loss (namely, A2-winner) significantly improves the performance on both high-dimensional (image) and low-dimensional (vector) unseen tasks with much higher efficiency, showing promising generalization ability to different settings and even different benchmark domains. We conduct a statistical analysis to reveal the relations between patterns of auxiliary losses and RL performance.",
    bibtex: "@inproceedings{zhao2021model,\n    title={Model-free safe control for zero-violation reinforcement learning},\n    author={Zhao, Weiye and He, Tairan and Liu, Changliu},\n    booktitle={5th Annual Conference on Robot Learning},\n    year={2021}\n  }",
    video: "https://tairanhe.com/images/ebil/ebil_heat_40.mp4",
  },
  {
    title: "Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion",
    authors: "Tairan He*, Xuxin Cheng*, Deepak Pathak",
    year: "",
    link: "https://manipulation-locomotion.github.io",
    webpage: "https://manipulation-locomotion.github.io",
    pdf: "https://arxiv.org/pdf/2210.10044.pdf",
    arxiv: "https://arxiv.org/abs/2210.10044",
    abstract: "An attached arm can significantly increase the applicability of legged robots to several mobile manipulation tasks that are not possible for the wheeled or tracked counterparts. The standard control pipeline for such legged manipulators is to decouple the controller into that of manipulation and locomotion. However, this is ineffective and requires immense engineering to support coordination between the arm and legs, error can propagate across modules causing non-smooth unnatural motions. It is also biological implausible where there is evidence for strong motor synergies across limbs. In this work, we propose to learn a unified policy for whole-body control of a legged manipulator using reinforcement learning. We propose Regularized Online Adaptation to bridge the Sim2Real gap for high-DoF control, and Advantage Mixing exploiting the causal dependency in the action space to overcome local minima during training the whole-body system. We also present a simple design for a low-cost legged manipulator, and find that our unified policy can demonstrate dynamic and agile behaviors across several task setups.",
    bibtex: "@inproceedings{fu2022deep,\n  author = {Fu, Zipeng and Cheng, Xuxin and \n            Pathak, Deepak},\n  title = {Deep Whole-Body Control: Learning a Unified Policy\n           for Manipulation and Locomotion},\n  booktitle = {Conference on Robot Learning ({CoRL})},\n  year = {2022}\n}",
    video: "https://tairanhe.com/images/wbc-clip.mp4",
  },
  {
    title: "SJTU Anonymous Forum 「无可奉告」",
    authors: "",
    year: "",
    link: "https://www.bilibili.com/video/BV1Rp4y187ZJ",
    abstract: "A carefree forum platform for SJTUers sharing and talking with anonymous identity. More than <font color=\"red\"><em><strong>10000+</strong></em></font> users used「无可奉告」in the SJTU campus.",
    image: "/index_files/wkfgicon.png",
  },
  {
    title: "WhynotTV Podcast",
    authors: "</p>\n\n      <div class=\"paper\" id=\"podcast\">\n\n        I host <a href=\"https://tairanhe.com/podcast_en\">WhynotTV Podcast</a> / <a href=\"https://tairanhe.com/podcast_cn\">WhynotTV播客</a>, a deep, professional, hardcore, long-form (2-4 hours) AI tech video podcast—focusing on",
    year: "",
    link: "https://tairanhe.com/podcast_en",
    image: "/index_files/whynottv.png",
  },
]
